{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768b51d1",
   "metadata": {},
   "source": [
    "### Below is a concise yet friendly explanation of the attention mechanism for assignment introduction:\n",
    "\n",
    "#### Attention Mechanism (Adapted from ‚ÄúAttention Is All You Need‚Äù)\n",
    "\n",
    "The attention mechanism, introduced in Attention Is All You Need (Vaswani et al., 2017), processes inputs represented as vectors (each row is a token embedding of dimension $ùê∑$). We compute three sets of vectors: queries (Q), keys (K), and values (V). The core steps are:\n",
    "\n",
    "1. **Linear Transformations:**  \n",
    "\n",
    "Let $X$ be the input matrix, where each of the rows corresponds to a token in the input sequence, and each row is a $d$-dimensional embedding vector.\n",
    "\n",
    "To compute attention, we first project $X$ into three different representations using learned weight matrices:\n",
    "\n",
    "Each input vector is transformed into $Q$, $K$, and $V$ using learnable weights.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_i &= X W_i^Q, \\\\\n",
    "K_i &= X W_i^K, \\\\\n",
    "V_i &= X W_i^V.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each head \\(i\\) has its own learnable parameters $W_i^Q$, $W_i^K$, and $W_i^V$, which transform the input into queries, keys, and values, respectively.\n",
    "\n",
    "\n",
    "2. **Scaled Dot-Product Attention:**  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
    "\\end{equation}\n",
    "\n",
    "Here, $QK^T$ produces a matrix of scores that measures how relevant each ‚Äúquery‚Äù position is to every ‚Äúkey‚Äù position. $d_k$ is the dimension of queries and keys. \n",
    "\n",
    "The softmax function converts these scores into attention weights (non-negative values that sum to 1 across each row).\n",
    "\n",
    "These weights are then used to combine the values ùëâ to produce the final output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. **Multi-Head Attention:**  \n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V).\n",
    "$$\n",
    "\n",
    "\n",
    "Multiple attention heads allow the model to attend to different aspects of the input simultaneously. For each of these heads we use $d_k = d_{model}/H$. Their outputs are concatenated and linearly transformed to produce the final result.\n",
    "\n",
    "\n",
    "\n",
    "*Note:* In this assignment, you are only required to experiment with the provided $Q$, $K$, and $V$ matrices to perform the matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ccbf8",
   "metadata": {},
   "source": [
    "## Self-attention Computer Assignment \n",
    "\n",
    "\n",
    "Implement the multi-head self-attention operation, taking in a set of $N$ vectors of $D$ dimensions and outputting a matrix of the same size. Do this without relying on neural network libraries, but rather write directly the required operations in NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434b6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6598d929-2af6-4d94-acee-1341af2a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "N = 5\n",
    "D = 6\n",
    "\n",
    "X = [[ 0.7, -0.8, -1.2,  -1.,  -0., -0.3],\n",
    "     [ 2.7,  0.1,  1.6,  1.8,  1.5,  0.3],\n",
    "     [ 0.1,  2.6, -0.1, -1.3, -0.5, -0.7],\n",
    "     [ 1.1,  1.5,   1., -0.5,  0.4,  0.4],\n",
    "     [-0.7, -0.7,  0.7, -1.5, -0.8,  1. ]]\n",
    "\n",
    "Wq = [[-1.7,  1.6,  0.9, -0.5,  0.4,  -1.],\n",
    "      [-0.4,  1. , -0.3,  1. ,  0.5,  1.1],\n",
    "      [ 0.4, -0.9,  -1.,  0.5, -1.4,  0. ],\n",
    "      [ 0.3,  1.4, -1.2,  0.2,  0.1,  1.6],\n",
    "      [-0.8,  0.8, -0.7, -1.3,  0.3,  0.8],\n",
    "      [ 1.1,  0.3, -1.5, -2.3,  2.2, -0.7]]\n",
    "\n",
    "Wk = [[ 0.3, -0.4, -1.3,  0.3, -1.7,  1.1],\n",
    "      [-2.3, -1.1,  0.6, -1.2,  2.2,  0.3],\n",
    "      [ 1.1, -0.4, -0.5,  1.9, -1.1, -1.2],\n",
    "      [-0.4,  1. , -1.7,  0. , -3.3, -1.4],\n",
    "      [-0.9, -1.1, -1. ,  1.4,  1.3,  1.2],\n",
    "      [-0.7,  0.4,  0.4, -1.4, -0.2, -0.5]]\n",
    "\n",
    "Wv = [[-0.1,  0.7,  1. , -0.1,  1.6,  0.9],\n",
    "      [ 0.4, -1. , -0.7, -0.6, -0.9, -0.1],\n",
    "      [-0.4,  0.5, -1.4,  0.1,  0.6,  0.4],\n",
    "      [ 1.4, -1.3, -1.3, -0.6,  1.6, -0.2],\n",
    "      [-0.4, -0.6, -1.4, -1. ,  0.4, -0.8],\n",
    "      [ 0.2,  0.5,  0.4, -0.5,  1.4,  2.3]]\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Wq = np.array(Wq)\n",
    "Wk = np.array(Wk)\n",
    "Wv = np.array(Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382714-52d9-4a53-9361-19e81d5b64c6",
   "metadata": {},
   "source": [
    "### (a) Implement the self-attention operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0314ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, Wq, Wk, Wv):\n",
    "\t...\n",
    "\tquery = X.dot(Wq)\n",
    "\tkey = X.dot(Wk)\n",
    "\tvalue = X.dot(Wv)\n",
    "\t\n",
    "\td_k = key.shape[1]\n",
    "\tscores = query.dot(key.T) / (d_k ** 0.5)\n",
    "\tattention_weights = np.exp(scores) / np.sum(np.exp(scores), axis = -1, keepdims = True)\n",
    "\toutput = attention_weights.dot(value)\n",
    "\treturn output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4298990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "Self-Attention Matrix:\n",
      " [[4.9e-07 4.0e-14 9.9e-01 1.7e-05 6.1e-03]\n",
      " [4.8e-01 3.6e-01 1.5e-01 2.1e-02 4.0e-04]\n",
      " [1.9e-02 5.8e-01 9.4e-02 2.8e-01 3.2e-02]\n",
      " [3.0e-02 1.2e-02 8.5e-01 9.9e-02 1.1e-02]\n",
      " [4.7e-04 7.3e-03 1.6e-02 4.0e-02 9.4e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the output\n",
    "output, attention_weights = self_attention(X, Wq, Wk, Wv)\n",
    "\n",
    "# Print in a nice format\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Self-Attention Output:\\n\", output)\n",
    "print(\"Self-Attention Matrix:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644c648-2f0a-4ae2-a50e-17d0c7f412bb",
   "metadata": {},
   "source": [
    "### (b) Implement multi-head attention, using the previously implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e149e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, Wq, Wk, Wv, H):\n",
    "\t...\n",
    "\thead_dim = Wq.shape[1] // H\n",
    "\toutputs = []\n",
    "\tattention_weights = []\n",
    "\t\n",
    "\tfor h in range(H):\n",
    "\t\tstart = h * head_dim\n",
    "\t\tend = (h+1) * head_dim\n",
    "\n",
    "\t\t# split by row for each head\n",
    "\t\tWq_h = Wq[:, start:end]\t\n",
    "\t\tWk_h = Wk[:, start:end]\n",
    "\t\tWv_h = Wv[:, start:end]\n",
    "\n",
    "\t\toutput_h, attention_weights_h = self_attention(X, Wq_h, Wk_h, Wv_h)\n",
    "\n",
    "\t\toutputs.append(output_h)\n",
    "\t\tattention_weights.append(attention_weights_h)\n",
    "\n",
    "\toutput = np.hstack(outputs)\n",
    "\tattention_weights = np.array(attention_weights)\n",
    "\treturn output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5563ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head number: 1, multi head attention output:\n",
      " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "head number: 1, multi head attention weights:\n",
      " [[[4.9e-07 4.0e-14 9.9e-01 1.7e-05 6.1e-03]\n",
      "  [4.8e-01 3.6e-01 1.5e-01 2.1e-02 4.0e-04]\n",
      "  [1.9e-02 5.8e-01 9.4e-02 2.8e-01 3.2e-02]\n",
      "  [3.0e-02 1.2e-02 8.5e-01 9.9e-02 1.1e-02]\n",
      "  [4.7e-04 7.3e-03 1.6e-02 4.0e-02 9.4e-01]]]\n",
      "head number: 3, multi head attention output:\n",
      " [[-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n",
      "head number: 3, multi head attention weights:\n",
      " [[[1.8e-04 1.2e-03 9.5e-01 4.6e-02 2.3e-05]\n",
      "  [5.1e-01 1.7e-02 3.6e-01 1.0e-02 1.0e-01]\n",
      "  [6.4e-04 2.5e-03 9.4e-01 5.4e-02 9.7e-05]\n",
      "  [3.8e-02 2.6e-02 8.4e-01 8.8e-02 1.0e-02]\n",
      "  [2.6e-02 3.2e-01 4.5e-02 5.3e-01 7.7e-02]]\n",
      "\n",
      " [[3.1e-05 2.0e-18 9.0e-01 4.8e-07 9.7e-02]\n",
      "  [6.1e-04 1.0e+00 9.6e-05 8.1e-04 9.0e-07]\n",
      "  [3.2e-03 1.1e-01 3.8e-04 1.5e-02 8.7e-01]\n",
      "  [1.0e-02 9.7e-01 2.3e-03 1.8e-02 2.0e-03]\n",
      "  [1.1e-01 7.5e-04 7.9e-01 4.0e-02 5.4e-02]]\n",
      "\n",
      " [[3.1e-05 3.9e-05 1.8e-02 3.6e-03 9.8e-01]\n",
      "  [4.3e-01 4.7e-03 4.9e-01 7.1e-02 6.8e-03]\n",
      "  [2.3e-01 6.7e-01 2.2e-02 6.3e-02 1.5e-02]\n",
      "  [1.4e-02 4.5e-05 9.1e-01 2.7e-02 4.4e-02]\n",
      "  [2.8e-06 3.9e-03 1.0e-04 8.4e-04 1.0e+00]]]\n"
     ]
    }
   ],
   "source": [
    "# Compute multi-head attention\n",
    "H = [1, 3]\n",
    "for head_number in H:\n",
    "\toutput, attention_weights = multi_head_attention(X, Wq, Wk, Wv, head_number)\n",
    "\tprint(f\"head number: {head_number}, multi head attention output:\\n {output}\")\n",
    "\tprint(f\"head number: {head_number}, multi head attention weights:\\n {attention_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c2abb-23bf-4772-9090-420aafac4639",
   "metadata": {},
   "source": [
    "### (c+d) Provide the answers/explanations requested in the problem sheet:\n",
    "1. Why the results are different?\n",
    "2. What happens if you change the order of two inputs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d6b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head number: 1, multi head attention output:\n",
      " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "head number: 1, multi head attention output reordered:\n",
      " [[-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "head number: 1, multi head attention weights:\n",
      " [[[4.9e-07 4.0e-14 9.9e-01 1.7e-05 6.1e-03]\n",
      "  [4.8e-01 3.6e-01 1.5e-01 2.1e-02 4.0e-04]\n",
      "  [1.9e-02 5.8e-01 9.4e-02 2.8e-01 3.2e-02]\n",
      "  [3.0e-02 1.2e-02 8.5e-01 9.9e-02 1.1e-02]\n",
      "  [4.7e-04 7.3e-03 1.6e-02 4.0e-02 9.4e-01]]]\n",
      "head number: 1, multi head attention weights reordered:\n",
      " [[[3.6e-01 4.8e-01 1.5e-01 2.1e-02 4.0e-04]\n",
      "  [4.0e-14 4.9e-07 9.9e-01 1.7e-05 6.1e-03]\n",
      "  [5.8e-01 1.9e-02 9.4e-02 2.8e-01 3.2e-02]\n",
      "  [1.2e-02 3.0e-02 8.5e-01 9.9e-02 1.1e-02]\n",
      "  [7.3e-03 4.7e-04 1.6e-02 4.0e-02 9.4e-01]]]\n",
      "head number: 3, multi head attention output:\n",
      " [[-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n",
      "head number: 3, multi head attention output reordered:\n",
      " [[-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n",
      "head number: 3, multi head attention weights:\n",
      " [[[1.8e-04 1.2e-03 9.5e-01 4.6e-02 2.3e-05]\n",
      "  [5.1e-01 1.7e-02 3.6e-01 1.0e-02 1.0e-01]\n",
      "  [6.4e-04 2.5e-03 9.4e-01 5.4e-02 9.7e-05]\n",
      "  [3.8e-02 2.6e-02 8.4e-01 8.8e-02 1.0e-02]\n",
      "  [2.6e-02 3.2e-01 4.5e-02 5.3e-01 7.7e-02]]\n",
      "\n",
      " [[3.1e-05 2.0e-18 9.0e-01 4.8e-07 9.7e-02]\n",
      "  [6.1e-04 1.0e+00 9.6e-05 8.1e-04 9.0e-07]\n",
      "  [3.2e-03 1.1e-01 3.8e-04 1.5e-02 8.7e-01]\n",
      "  [1.0e-02 9.7e-01 2.3e-03 1.8e-02 2.0e-03]\n",
      "  [1.1e-01 7.5e-04 7.9e-01 4.0e-02 5.4e-02]]\n",
      "\n",
      " [[3.1e-05 3.9e-05 1.8e-02 3.6e-03 9.8e-01]\n",
      "  [4.3e-01 4.7e-03 4.9e-01 7.1e-02 6.8e-03]\n",
      "  [2.3e-01 6.7e-01 2.2e-02 6.3e-02 1.5e-02]\n",
      "  [1.4e-02 4.5e-05 9.1e-01 2.7e-02 4.4e-02]\n",
      "  [2.8e-06 3.9e-03 1.0e-04 8.4e-04 1.0e+00]]]\n",
      "head number: 3, multi head attention weights reordered:\n",
      " [[[1.7e-02 5.1e-01 3.6e-01 1.0e-02 1.0e-01]\n",
      "  [1.2e-03 1.8e-04 9.5e-01 4.6e-02 2.3e-05]\n",
      "  [2.5e-03 6.4e-04 9.4e-01 5.4e-02 9.7e-05]\n",
      "  [2.6e-02 3.8e-02 8.4e-01 8.8e-02 1.0e-02]\n",
      "  [3.2e-01 2.6e-02 4.5e-02 5.3e-01 7.7e-02]]\n",
      "\n",
      " [[1.0e+00 6.1e-04 9.6e-05 8.1e-04 9.0e-07]\n",
      "  [2.0e-18 3.1e-05 9.0e-01 4.8e-07 9.7e-02]\n",
      "  [1.1e-01 3.2e-03 3.8e-04 1.5e-02 8.7e-01]\n",
      "  [9.7e-01 1.0e-02 2.3e-03 1.8e-02 2.0e-03]\n",
      "  [7.5e-04 1.1e-01 7.9e-01 4.0e-02 5.4e-02]]\n",
      "\n",
      " [[4.7e-03 4.3e-01 4.9e-01 7.1e-02 6.8e-03]\n",
      "  [3.9e-05 3.1e-05 1.8e-02 3.6e-03 9.8e-01]\n",
      "  [6.7e-01 2.3e-01 2.2e-02 6.3e-02 1.5e-02]\n",
      "  [4.5e-05 1.4e-02 9.1e-01 2.7e-02 4.4e-02]\n",
      "  [3.9e-03 2.8e-06 1.0e-04 8.4e-04 1.0e+00]]]\n"
     ]
    }
   ],
   "source": [
    "X_reordered = np.array([\n",
    "    [ 2.7,  0.1,  1.6,  1.8,  1.5,  0.3],\n",
    "\t[ 0.7, -0.8, -1.2,  -1.,  -0., -0.3],\n",
    "    [ 0.1,  2.6, -0.1, -1.3, -0.5, -0.7],\n",
    "    [ 1.1,  1.5,   1., -0.5,  0.4,  0.4],\n",
    "    [-0.7, -0.7,  0.7, -1.5, -0.8,  1. ]\n",
    "])\n",
    "\n",
    "# Compute multi-head attention\n",
    "H = [1, 3]\n",
    "for head_number in H:\n",
    "\toutput_reordered, attention_weights_reordered = multi_head_attention(X_reordered, Wq, Wk, Wv, head_number)\n",
    "\toutput, attention_weights = multi_head_attention(X, Wq, Wk, Wv, head_number)\n",
    "\tprint(f\"head number: {head_number}, multi head attention output:\\n {output}\")\n",
    "\tprint(f\"head number: {head_number}, multi head attention output reordered:\\n {output_reordered}\")\n",
    "\tprint(f\"head number: {head_number}, multi head attention weights:\\n {attention_weights}\")\n",
    "\tprint(f\"head number: {head_number}, multi head attention weights reordered:\\n {attention_weights_reordered}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f66e51",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "1. When using the multi head method we split the weight matrices into smaller matrices with same number of columns for each head but we process the same input resulting into a mechanims that allows different heads to attend to different parts of the input. Even though we use the same matrices in the multi head implementation each head is allowed to use only a certain partition of each matrix $$\\begin{aligned} Q_i &= X W_i^Q, \\\\ K_i &= X W_i^K, \\\\ V_i &= X W_i^V. \\end{aligned} $$ resulting in each head projecting into an **independent 2D subspace** and computing the attention separately in each. Finally each individual output is concatenated to form the final result $$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)$$ This process is very different from the single head implementation where one single head can attend the full input with the full dimentionality of the weight matrices. In conclusion there is no single operation that allows us to pinpoint where the two results break apart but we can say with certainty that the two results differ because of compound effect of:\t1. lower dimensional projections\t2. separate attention weight matrices\t3. independent softmax operations\t4. concatenation of final head outputs. We can say with certainty that the more expressive model is the multi head method because each head can extract different information from the input compared to only one head (eg. instead of only one perspective about the input, each head computes a different perspective equally weighted in the final output).\n",
    "2. When changing the order of the first two inputs we can notice that for the single head implementation the final output is the same, although the weights matrices are different. This happens because we are only relying on one attention head and the operations inside one single head would simply permutate the outputs in the same order as the rearrangement of the input (eg. $X' = \\begin{bmatrix} x_1 \\\\ x_0 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ \\end{bmatrix}$ would give us $Q' = \\begin{bmatrix} q_1 \\\\ q_0 \\\\ q_2 \\\\ q_3 \\\\ q_4 \\\\ \\end{bmatrix}$, this process is extended to the final output $Output' = P \\cdot Output$ where P is a permutation matrix that rearranges the rows). On the other hand in the multi head implementation we see that the output is different from the original multi head output, this happens because we are now relying on multiple heads that compute their own attention matrices for different parts of the input and compute a final output that is the concatenation of the single outputs. Since the result is a concatenation of different feature projections reordering the input results in a different mix of subspace outputs making the final result not just a permutation but a totally different output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
