{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Mathematical exercise 2 - Scientific article summary\n",
    "\n",
    "title: Structure-Aware Transformer for Graph Representation Learning  \n",
    "author names: Dexiong Chen, Leslie O'Bray, Karsten Borgwardt  \n",
    "url: [https://arxiv.org/abs/2202.03036](https://arxiv.org/abs/2202.03036)  \n",
    "\n",
    "## What is the task?\n",
    "\n",
    "The goal of this scientific article is to improve the performance of Transformers on graphs. Standard Transformers treat inputs as unordered sets or sequences therefore the attention mechanism can't learn the underlying graph structure. This model modifies attention to incorporate structural information such as distances between nodes and subgraph topology. The tasks used to evaluate the model include node classification and graph classification on benchmark datasets.\n",
    "\n",
    "\n",
    "## How is the input data processed?\n",
    "\n",
    "The data is graph structured where each sample is a graph \\( G = (V, E, X) \\), where:\n",
    "\n",
    "- V = nodes  \n",
    "- E = edges  \n",
    "- X = node features\n",
    "\n",
    "The paper explains how to extract small subgraphs around each node to represent the local structure. These subgraphs are then passed into the Transformer, so the model learns not just from the features but also from how the nodes are connected.\n",
    "\n",
    "## What kind of architecture was used? \n",
    "\n",
    "The model is a Transformer, but tweaked to handle graphs better. The key idea is the Structure-Aware Attention, which modifies the attention scores to take into account structural information between nodes (like graph distance or edge types).\n",
    "\n",
    "The model is made of:\n",
    "\n",
    "- Transformer layers (arbitrary number that varies by experiment)\n",
    "- A structure bias matrix that is added to the regular attention scores\n",
    "\n",
    "The exact number of parameters is not fixed but depends on dataset and implementation size.\n",
    "\n",
    "\n",
    "## How was the training done?\n",
    "\n",
    "The training is done in supervised fashion where we have node classification and graph classification tasks. The benchmark datasets used are graph datasets like PROTEINS, NCI1, and ENZYMES.\n",
    "\n",
    "Pretraining or transfer learning are not used in this paper but everything is trained from scratch.\n",
    "\n",
    "## Did it work well?\n",
    "\n",
    "In most experiments, their model outperforms standard baselines like:\n",
    "\n",
    "- GCN (Graph Convolutional Network)\n",
    "- GIN (Graph Isomorphism Network)\n",
    "- GraphTransformer (vanilla Transformer on graphs)\n",
    "\n",
    "We can see improvements especially on tasks where graph structure is important. The real improvement percentages are not given in the paper.\n",
    "\n",
    "## Your subjective feeling about the paper: Is the model good?\n",
    "\n",
    "The idea of modelling graph data with transformers is not bad since graph structure is hard to learn. This paper tries to improve performance of the standard Transformer without breaking the Transformer format. The core idea of infusing structural information into attention computation via relational encodings is both intuitive and effective. The paper is pretty clear, with many figures and intuitive explanations for the structure bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
